{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"notebook\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import imageio\n",
    "import numpy as np\n",
    "from IPython.display import Image as ImageDisplay\n",
    "\n",
    "PATH = os.getcwd()\n",
    "TAG = \"hf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load input image to lift to 3D (multiple objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA devices:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LIDRA_SKIP_INIT\"] = \"true\"\n",
    "\n",
    "from pathlib import Path\n",
    "from sam3d_objects.pipeline.inference_with_embeddings import InferenceWithEmbeddings\n",
    "\n",
    "\n",
    "config_path = f\"{PATH}/../checkpoints/{TAG}/pipeline.yaml\"\n",
    "\n",
    "data_dir = Path(\"../../../../../projects/FRI/jn16867/3d-counting/Stacks-3D-Real/scenes\")\n",
    "\n",
    "pipeline = InferenceWithEmbeddings(config_path, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import sam3d_objects.data.precompute_embeddings as pe\n",
    "\n",
    "reload(pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category_dir in sorted(data_dir.iterdir()):\n",
    "    if (category_dir / \"geco2_mask\" / \"image.png\").exists():\n",
    "        print(f\"Processing sample for category: {category_dir}\")\n",
    "        pe.preprocess_sample(category_dir, pipeline)\n",
    "    else:\n",
    "        print(f\"No GeCo2 mask found for category: {category_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "train_dir_1 = Path(\"../../../../../projects/FRI/jn16867/3d-counting/scenes_part1\")\n",
    "train_dir_2 = Path(\"../../../../../projects/FRI/jn16867/3d-counting/scenes_part2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(train_dir_1 / \"00003219_1\" / \"00003219_1.npz\")\n",
    "print(data.files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "image = np.array(Image.open(train_dir_1 / \"00003531_1\" / \"images\" / \"RGB0010.jpg\").convert(\"RGB\"))\n",
    "box = np.array(Image.open(train_dir_1 / \"00003531_1\" / \"box_seg\" / \"Box_Mask0010.png\").convert(\"RGB\"))\n",
    "obj = np.array(Image.open(train_dir_1 / \"00003531_1\" / \"obj_seg\" / \"Objects_Mask0010.png\").convert(\"RGB\"))\n",
    "floor = np.array(Image.open(train_dir_1 / \"00003531_1\" / \"floor_seg\" / \"Ground_Mask0010.png\").convert(\"RGB\"))\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(obj)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LIDRA_SKIP_INIT\"] = \"true\"\n",
    "\n",
    "from sam3d_objects.pipeline.inference_with_embeddings import InferenceWithEmbeddings\n",
    "\n",
    "config_path = f\"{PATH}/../checkpoints/{TAG}/pipeline.yaml\"\n",
    "pipeline = InferenceWithEmbeddings(config_path, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import json\n",
    "\n",
    "\n",
    "frame = \"0010\"\n",
    "data_dir = Path(\"../../../../../projects/FRI/jn16867/3d-counting/scenes_part1\")\n",
    "\n",
    "for scene in sorted(data_dir.iterdir()):\n",
    "    image = np.array(Image.open(scene / \"images\" / (\"RGB\" + frame + \".jpg\")).convert(\"RGB\"))\n",
    "    box_mask = np.array(Image.open(scene / \"box_seg\" / (\"Box_Mask\" + frame + \".png\")).convert(\"L\"))\n",
    "    obj_mask = np.array(Image.open(scene / \"obj_seg\" / (\"Objects_Mask\" + frame + \".png\")).convert(\"L\"))\n",
    "    \n",
    "    floor_mask = Image.open(scene / \"floor_seg\" / (\"Ground_Mask\" + frame + \".png\")).convert(\"L\")\n",
    "    container_mask = np.array(ImageOps.invert(floor_mask))\n",
    "    \n",
    "    obj_mask = (obj_mask > 0).astype(np.uint8) * 255\n",
    "    container_mask = (container_mask > 0).astype(np.uint8) * 255\n",
    "\n",
    "    print(f\"Computing embeddings for container\")\n",
    "    container_out = pipeline.run_with_embeddings(image, container_mask, seed=42)\n",
    "    print(f\"Computing embeddings for object\")\n",
    "    object_out = pipeline.run_with_embeddings(image, obj_mask, seed=42)\n",
    "    \n",
    "    with open(scene / \"gt_count.json\") as f:\n",
    "        gt_count = json.load(f)\n",
    "        \n",
    "    save_dict = {\n",
    "        \"container\": container_out,\n",
    "        \"object\": object_out,\n",
    "        \"true_count\": gt_count\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, data_dir / \"embeddings.pt\")\n",
    "    print(f\"Saved embeddings for {data_dir.name}\")\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(scene / \"gt_count.json\") as f:\n",
    "    gt_count = json.load(f)\n",
    "    print(gt_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(floor)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array(Image.open(train_dir_1 / \"00003851_0\" / \"image.png\").convert(\"RGB\"))\n",
    "first_frame = \"frame_00001.png\"\n",
    "\n",
    "if (data_dir / \"box_seg\").exists():\n",
    "    container_mask = np.array(Image.open(data_dir / \"box_seg\" / first_frame).convert(\"L\"))\n",
    "elif (data_dir / \"obj_seg\").exists():\n",
    "    container_mask = np.array(Image.open(data_dir / \"obj_seg\" / first_frame).convert(\"L\"))\n",
    "else:\n",
    "    print(f\"No box or object segmentation found for data_dir: {data_dir}. Skipping category.\")\n",
    "container_mask = (container_mask > 0).astype(np.uint8) * 255\n",
    "\n",
    "object_mask = np.array(Image.open(data_dir / \"geco2_mask\" / \"mask.png\"))\n",
    "object_mask = (object_mask > 0).astype(np.uint8) * 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sam3d_objects.data.count_dataset import CountDataset\n",
    "\n",
    "\n",
    "data_dir = Path(\"../../../../../projects/FRI/jn16867/3d-counting/Stacks-3D-Real/scenes\")\n",
    "\n",
    "dataset = CountDataset(data_dir)\n",
    "print(f\"Dataset length: {len(dataset)}\")\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    collate_fn=lambda x: x[0]\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    collate_fn=lambda x: x[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam3d_objects.pipeline.sam3d_count_predictor import SAM3DCountPredictor\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\"\n",
    "num_epochs = 100\n",
    "lr = 1e-3\n",
    "output_dir = Path(\"../model_checkpoints/stacks-3d/\")\n",
    "\n",
    "use_hybrid = False\n",
    "model = SAM3DCountPredictor(use_hybrid=use_hybrid).to(device)\n",
    "    \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "l1_loss = nn.L1Loss()\n",
    "\n",
    "best_val_mae = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best current model and continue training\n",
    "checkpoint = torch.load(Path(\"../model_checkpoints/stacks-3d/checkpoint_epoch_100.pth\"), map_location=device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "start_epoch = checkpoint[\"epoch\"] + 1\n",
    "best_val_mae = checkpoint[\"best_val_mae\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sam3d_objects.pipeline.inference_with_embeddings import extract_geometric_features, compute_geometric_count_estimate\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_mae = 0\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs} - Training...\")\n",
    "    \n",
    "    for sample in tqdm(train_loader):\n",
    "        container_out = sample['container_outputs']\n",
    "        object_out = sample['object_outputs']\n",
    "        true_count = torch.tensor([sample['true_count']], dtype=torch.float32).to(device)\n",
    "        \n",
    "        shape_latent_container = container_out['shape_latent'].to(device)\n",
    "        shape_latent_object = object_out['shape_latent'].to(device)\n",
    "            \n",
    "        slat_features_container = container_out['slat_features'].to(device)\n",
    "        slat_features_object = object_out['slat_features'].to(device)\n",
    "        \n",
    "        if slat_features_container.dim() == 2:\n",
    "            slat_features_container = slat_features_container.unsqueeze(0)\n",
    "        if slat_features_object.dim() == 2:\n",
    "            slat_features_object = slat_features_object.unsqueeze(0)\n",
    "        \n",
    "        geom_feat = extract_geometric_features(container_out, object_out).unsqueeze(0).to(device)\n",
    "        \n",
    "        geometric_estimate = torch.tensor(\n",
    "            [compute_geometric_count_estimate(container_out, object_out, 1.0)],\n",
    "            dtype=torch.float32\n",
    "        ).to(device)\n",
    "        \n",
    "        if use_hybrid:\n",
    "            pred_count, correction = model(\n",
    "                shape_latent_container,\n",
    "                shape_latent_object,\n",
    "                slat_features_container,\n",
    "                slat_features_object,\n",
    "                geom_feat,\n",
    "                geometric_estimate\n",
    "            )\n",
    "        else:\n",
    "            pred_count, _ = model(\n",
    "                shape_latent_container,\n",
    "                shape_latent_object,\n",
    "                slat_features_container,\n",
    "                slat_features_object,\n",
    "                geom_feat,\n",
    "                None\n",
    "            )\n",
    "            \n",
    "        loss = 0.5 * mse_loss(pred_count, true_count) + 0.5 * l1_loss(pred_count, true_count)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_mae += torch.abs(pred_count - true_count).item()\n",
    "        \n",
    "    print(f\"Train loss at epoch {epoch + 1}: {train_loss}\")\n",
    "    print(f\"Train mae at epoch {epoch + 1}: {train_mae}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_dataset)\n",
    "    avg_train_mae = train_mae / len(train_dataset)\n",
    "\n",
    "    # Validation\n",
    "    if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_mae = 0\n",
    "        print(\"Validating\")\n",
    "        with torch.no_grad():\n",
    "            for sample in tqdm(val_loader):\n",
    "                container_out = sample['container_outputs']\n",
    "                object_out = sample['object_outputs']\n",
    "                true_count = torch.tensor([sample['true_count']], dtype=torch.float32).to(device)\n",
    "                \n",
    "                shape_latent_container = container_out['shape_latent'].to(device)\n",
    "                shape_latent_object = object_out['shape_latent'].to(device)\n",
    "                    \n",
    "                slat_features_container = container_out['slat_features'].to(device)\n",
    "                slat_features_object = object_out['slat_features'].to(device)\n",
    "                \n",
    "                if slat_features_container.dim() == 2:\n",
    "                    slat_features_container = slat_features_container.unsqueeze(0)\n",
    "                if slat_features_object.dim() == 2:\n",
    "                    slat_features_object = slat_features_object.unsqueeze(0)\n",
    "                \n",
    "                geom_feat = extract_geometric_features(container_out, object_out).unsqueeze(0).to(device)\n",
    "                \n",
    "                geometric_estimate = torch.tensor(\n",
    "                    [compute_geometric_count_estimate(container_out, object_out, 1.0)],\n",
    "                    dtype=torch.float32\n",
    "                ).to(device)\n",
    "                \n",
    "                pred_count, _ = model(\n",
    "                    shape_latent_container,\n",
    "                    shape_latent_object,\n",
    "                    slat_features_container,\n",
    "                    slat_features_object,\n",
    "                    geom_feat,\n",
    "                    None\n",
    "                )\n",
    "                \n",
    "                loss = 0.5 * mse_loss(pred_count, true_count) + 0.5 * l1_loss(pred_count, true_count)\n",
    "                \n",
    "                val_loss += loss\n",
    "                val_mae += torch.abs(pred_count - true_count).item()\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_dataset)\n",
    "            avg_val_mae = val_mae / len(val_dataset)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}, Train MAE: {avg_train_mae:.2f}\")\n",
    "            print(f\"  Val Loss: {avg_val_loss:.4f}, Val MAE: {avg_val_mae:.2f}\")\n",
    "            \n",
    "            if avg_val_mae < best_val_mae:\n",
    "                best_val_mae = avg_val_mae\n",
    "                \n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'best_val_mae': best_val_mae \n",
    "                }, output_dir / 'best_model.pth')\n",
    "                print(f\"  Saved new best model with VAL MAE: {avg_val_mae:.2f}\")\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, output_dir / f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "        \n",
    "print(\"\\nTraining completed\")\n",
    "print(f\"Best Validation MAE: {best_val_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/hpc/home/jn16867/.conda/envs/sam3d-objects/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "\u001b[32m2026-02-20 15:50:06.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mset_attention_backend\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mGPU name is Tesla V100S-PCIE-32GB\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 15:52:39.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.tdfy_dit.modules.sparse\u001b[0m:\u001b[36m__from_env\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1m[SPARSE] Backend: spconv, Attention: sdpa\u001b[0m\n",
      "\u001b[32m2026-02-20 15:52:57.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.tdfy_dit.modules.attention\u001b[0m:\u001b[36m__from_env\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1m[ATTENTION] Using backend: sdpa\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPARSE][CONV] spconv algo: auto\n",
      "Warp 1.11.0 initialized:\n",
      "   CUDA Toolkit 12.9, Driver 13.0\n",
      "   Devices:\n",
      "     \"cpu\"      : \"x86_64\"\n",
      "     \"cuda:0\"   : \"Tesla V100S-PCIE-32GB\" (32 GiB, sm_70, mempool enabled)\n",
      "     \"cuda:1\"   : \"Tesla V100S-PCIE-32GB\" (32 GiB, sm_70, mempool enabled)\n",
      "   CUDA peer access:\n",
      "     Supported fully (all-directional)\n",
      "   Kernel cache:\n",
      "     /d/hpc/home/jn16867/.cache/warp/1.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 15:54:45.619\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36msam3d_objects.data.dataset.tdfy.preprocessor\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m51\u001b[0m - \u001b[33m\u001b[1mNo rgb pointmap normalizer provided, using scale + shift \u001b[0m\n",
      "\u001b[32m2026-02-20 15:54:45.620\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36msam3d_objects.data.dataset.tdfy.preprocessor\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m51\u001b[0m - \u001b[33m\u001b[1mNo rgb pointmap normalizer provided, using scale + shift \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LIDRA_SKIP_INIT\"] = \"true\"\n",
    "\n",
    "from sam3d_objects.pipeline.inference_with_embeddings import InferenceWithEmbeddings\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/d/hpc/home/jn16867/cso/sam-3d-objects/notebook'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"cso/sam-3d-objects/notebook\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 18:33:43.105\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36msam3d_objects.data.dataset.tdfy.preprocessor\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m51\u001b[0m - \u001b[33m\u001b[1mNo rgb pointmap normalizer provided, using scale + shift \u001b[0m\n",
      "/d/hpc/home/jn16867/.conda/envs/sam3d-objects/lib/python3.11/site-packages/moge/model/v1.py:171: UserWarning: The following deprecated/invalid arguments are ignored: {'output_mask': True, 'split_head': True}\n",
      "  warnings.warn(f\"The following deprecated/invalid arguments are ignored: {deprecated_kwargs}\")\n",
      "\u001b[32m2026-02-20 18:34:08.097\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36msam3d_objects.data.dataset.tdfy.preprocessor\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m51\u001b[0m - \u001b[33m\u001b[1mNo rgb pointmap normalizer provided, using scale + shift \u001b[0m\n",
      "\u001b[32m2026-02-20 18:34:08.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m100\u001b[0m - \u001b[1mself.device: cuda\u001b[0m\n",
      "\u001b[32m2026-02-20 18:34:08.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m101\u001b[0m - \u001b[1mCUDA_VISIBLE_DEVICES: 0,1\u001b[0m\n",
      "\u001b[32m2026-02-20 18:34:08.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mActually using GPU: 0\u001b[0m\n",
      "\u001b[32m2026-02-20 18:34:08.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36minit_pose_decoder\u001b[0m:\u001b[36m297\u001b[0m - \u001b[1mUsing pose decoder: ScaleShiftInvariant\u001b[0m\n",
      "\u001b[32m2026-02-20 18:34:08.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1mLoading model weights...\u001b[0m\n",
      "\u001b[32m2026-02-20 18:34:11.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.io\u001b[0m:\u001b[36mload_model_from_checkpoint\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mLoading checkpoint from /d/hpc/home/jn16867/cso/sam-3d-objects/notebook/../checkpoints/hf/ss_generator.ckpt\u001b[0m\n",
      "\u001b[32m2026-02-20 18:35:29.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.io\u001b[0m:\u001b[36mload_model_from_checkpoint\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mLoading checkpoint from /d/hpc/home/jn16867/cso/sam-3d-objects/notebook/../checkpoints/hf/slat_generator.ckpt\u001b[0m\n",
      "\u001b[32m2026-02-20 18:36:27.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.io\u001b[0m:\u001b[36mload_model_from_checkpoint\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mLoading checkpoint from /d/hpc/home/jn16867/cso/sam-3d-objects/notebook/../checkpoints/hf/ss_decoder.ckpt\u001b[0m\n",
      "\u001b[32m2026-02-20 18:36:30.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.io\u001b[0m:\u001b[36mload_model_from_checkpoint\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mLoading checkpoint from /d/hpc/home/jn16867/cso/sam-3d-objects/notebook/../checkpoints/hf/slat_decoder_gs.ckpt\u001b[0m\n",
      "\u001b[32m2026-02-20 18:36:32.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.io\u001b[0m:\u001b[36mload_model_from_checkpoint\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mLoading checkpoint from /d/hpc/home/jn16867/cso/sam-3d-objects/notebook/../checkpoints/hf/slat_decoder_gs_4.ckpt\u001b[0m\n",
      "\u001b[32m2026-02-20 18:36:35.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.io\u001b[0m:\u001b[36mload_model_from_checkpoint\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mLoading checkpoint from /d/hpc/home/jn16867/cso/sam-3d-objects/notebook/../checkpoints/hf/slat_decoder_mesh.ckpt\u001b[0m\n",
      "\u001b[32m2026-02-20 18:36:37.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.dit.embedder.dino\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mLoading DINO model: dinov2_vitl14_reg from facebookresearch/dinov2 (source: github)\u001b[0m\n",
      "\u001b[32m2026-02-20 18:36:56.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.dit.embedder.dino\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mLoaded DINO model - type: <class 'dinov2.models.vision_transformer.DinoVisionTransformer'>, embed_dim: 1024, patch_size: (14, 14)\u001b[0m\n",
      "\u001b[32m2026-02-20 18:36:56.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.dit.embedder.dino\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mLoading DINO model: dinov2_vitl14_reg from facebookresearch/dinov2 (source: github)\u001b[0m\n",
      "\u001b[32m2026-02-20 18:36:56.968\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.dit.embedder.dino\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mLoaded DINO model - type: <class 'dinov2.models.vision_transformer.DinoVisionTransformer'>, embed_dim: 1024, patch_size: (14, 14)\u001b[0m\n",
      "\u001b[32m2026-02-20 18:36:56.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.io\u001b[0m:\u001b[36mload_model_from_checkpoint\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mLoading checkpoint from /d/hpc/home/jn16867/cso/sam-3d-objects/notebook/../checkpoints/hf/ss_generator.ckpt\u001b[0m\n",
      "\u001b[32m2026-02-20 18:36:59.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.dit.embedder.dino\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mLoading DINO model: dinov2_vitl14_reg from facebookresearch/dinov2 (source: github)\u001b[0m\n",
      "\u001b[32m2026-02-20 18:37:00.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.dit.embedder.dino\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mLoaded DINO model - type: <class 'dinov2.models.vision_transformer.DinoVisionTransformer'>, embed_dim: 1024, patch_size: (14, 14)\u001b[0m\n",
      "\u001b[32m2026-02-20 18:37:00.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.dit.embedder.dino\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mLoading DINO model: dinov2_vitl14_reg from facebookresearch/dinov2 (source: github)\u001b[0m\n",
      "\u001b[32m2026-02-20 18:37:00.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.dit.embedder.dino\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mLoaded DINO model - type: <class 'dinov2.models.vision_transformer.DinoVisionTransformer'>, embed_dim: 1024, patch_size: (14, 14)\u001b[0m\n",
      "\u001b[32m2026-02-20 18:37:00.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.io\u001b[0m:\u001b[36mload_model_from_checkpoint\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mLoading checkpoint from /d/hpc/home/jn16867/cso/sam-3d-objects/notebook/../checkpoints/hf/slat_generator.ckpt\u001b[0m\n",
      "\u001b[32m2026-02-20 18:37:02.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36moverride_ss_generator_cfg_config\u001b[0m:\u001b[36m438\u001b[0m - \u001b[1mss_generator parameters: inference_steps=25, cfg_strength=7, cfg_interval=[0, 500], rescale_t=3, cfg_strength_pm=0.0\u001b[0m\n",
      "\u001b[32m2026-02-20 18:37:02.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36moverride_slat_generator_cfg_config\u001b[0m:\u001b[36m460\u001b[0m - \u001b[1mslat_generator parameters: inference_steps=25, cfg_strength=1, cfg_interval=[0, 500], rescale_t=1\u001b[0m\n",
      "\u001b[32m2026-02-20 18:37:02.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mLoading model weights completed!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "PATH = os.getcwd()\n",
    "TAG = \"hf\"\n",
    "\n",
    "config_path = f\"{PATH}/../checkpoints/{TAG}/pipeline.yaml\"\n",
    "pipeline = InferenceWithEmbeddings(config_path, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_dir: set, pipeline: InferenceWithEmbeddings):\n",
    "    for scene in sorted(data_dir.iterdir()):\n",
    "        if not os.path.exists(scene / \"geco2_data\"):\n",
    "            print(f\"Skipping scene {scene}, no geco2_data found\")\n",
    "            continue\n",
    "        \n",
    "        image = np.array(Image.open(scene / \"geco2_data\" / \"image.png\").convert(\"RGB\"))\n",
    "        obj_mask = np.array(Image.open(scene / \"geco2_data\" / \"obj_mask.png\").convert(\"L\"))\n",
    "        box_mask = np.array(Image.open(scene / \"geco2_data\" / \"box_mask.png\").convert(\"L\"))\n",
    "        \n",
    "        obj_mask = (obj_mask > 0).astype(np.uint8)\n",
    "        box_mask = (box_mask > 0).astype(np.uint8)\n",
    "        \n",
    "        object_out = pipeline.run_with_embeddings(image, obj_mask, seed=42)\n",
    "        container_out = pipeline.run_with_embeddings(image, box_mask, seed=42)\n",
    "        \n",
    "        with open(scene / \"gt_count.json\") as f:\n",
    "            gt_count = json.load(f)\n",
    "        \n",
    "        save_dict = {\n",
    "            \"container\": container_out,\n",
    "            \"object\": object_out,\n",
    "            \"true_count\": gt_count\n",
    "        }\n",
    "        \n",
    "        torch.save(save_dict, scene / \"embeddings.pt\")\n",
    "        print(f\"Saved embeddings for {scene.name}\")\n",
    "        \n",
    "    return\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 19:00:57.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mmerge_image_and_mask\u001b[0m:\u001b[36m584\u001b[0m - \u001b[1mReplacing alpha channel with the provided mask\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPreparing data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m start_time = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m end_time = time.time() - start_time\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinished preparing data, time taken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mprepare_data\u001b[39m\u001b[34m(data_dir, pipeline)\u001b[39m\n\u001b[32m     11\u001b[39m obj_mask = (obj_mask > \u001b[32m0\u001b[39m).astype(np.uint8)\n\u001b[32m     12\u001b[39m box_mask = (box_mask > \u001b[32m0\u001b[39m).astype(np.uint8)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m object_out = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_with_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m container_out = pipeline.run_with_embeddings(image, box_mask, seed=\u001b[32m42\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(scene / \u001b[33m\"\u001b[39m\u001b[33mgt_count.json\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cso/sam-3d-objects/sam3d_objects/pipeline/inference_with_embeddings.py:149\u001b[39m, in \u001b[36mInferenceWithEmbeddings.run_with_embeddings\u001b[39m\u001b[34m(self, image, mask, seed, stage1_inference_steps, stage2_inference_steps)\u001b[39m\n\u001b[32m    146\u001b[39m pointmap_dict = \u001b[38;5;28mself\u001b[39m._pipeline.compute_pointmap(image)\n\u001b[32m    147\u001b[39m pointmap = pointmap_dict[\u001b[33m\"\u001b[39m\u001b[33mpointmap\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m ss_input_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mss_preprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpointmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpointmap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m slat_input_dict = \u001b[38;5;28mself\u001b[39m._pipeline.preprocess_image(image, \u001b[38;5;28mself\u001b[39m._pipeline.slat_preprocessor, pointmap=pointmap)\n\u001b[32m    152\u001b[39m torch.manual_seed(seed)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cso/sam-3d-objects/sam3d_objects/pipeline/inference_pipeline_pointmap.py:192\u001b[39m, in \u001b[36mInferencePipelinePointMap.preprocess_image\u001b[39m\u001b[34m(self, image, preprocessor, pointmap)\u001b[39m\n\u001b[32m    189\u001b[39m rgb_image = rgba_image[:\u001b[32m3\u001b[39m]\n\u001b[32m    190\u001b[39m rgb_image_mask = get_mask(rgba_image, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mALPHA_CHANNEL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m preprocessor_return_dict = \u001b[43mpreprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_process_image_mask_pointmap_mess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrgb_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrgb_image_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpointmap\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# Put in a for loop?\u001b[39;00m\n\u001b[32m    197\u001b[39m _item = preprocessor_return_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cso/sam-3d-objects/sam3d_objects/data/dataset/tdfy/preprocessor.py:81\u001b[39m, in \u001b[36mPreProcessor._process_image_mask_pointmap_mess\u001b[39m\u001b[34m(self, rgb_image, rgb_image_mask, pointmap)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Extended version that handles pointmaps\"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Apply pointmap normalization if enabled\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m pointmap_for_crop, pointmap_scale, pointmap_shift = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_normalize_pointmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpointmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrgb_image_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpointmap_normalizer\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Apply transforms to the original full rgb image and mask.\u001b[39;00m\n\u001b[32m     86\u001b[39m rgb_image, rgb_image_mask = \u001b[38;5;28mself\u001b[39m._preprocess_rgb_image_mask(rgb_image, rgb_image_mask)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cso/sam-3d-objects/sam3d_objects/data/dataset/tdfy/preprocessor.py:73\u001b[39m, in \u001b[36mPreProcessor._normalize_pointmap\u001b[39m\u001b[34m(self, pointmap, mask, pointmap_normalizer, scale, shift)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m shift \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pointmap_normalizer.normalize(pointmap, mask, scale, shift)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpointmap_normalizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpointmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cso/sam-3d-objects/sam3d_objects/data/dataset/tdfy/img_and_mask_transforms.py:597\u001b[39m, in \u001b[36mObjectCentricSSI.normalize\u001b[39m\u001b[34m(self, pointmap, mask, scale, shift)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m pointmap.shape[\u001b[32m0\u001b[39m] == \u001b[32m3\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpointmap must be in (3, H, W) format\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    595\u001b[39m pointmap_size = (pointmap.shape[\u001b[32m1\u001b[39m], pointmap.shape[\u001b[32m2\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m597\u001b[39m _scale, _shift = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_scale_and_shift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpointmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.allow_scale_and_shift_override:\n\u001b[32m    599\u001b[39m     _scale = scale\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cso/sam-3d-objects/sam3d_objects/data/dataset/tdfy/img_and_mask_transforms.py:550\u001b[39m, in \u001b[36mObjectCentricSSI._compute_scale_and_shift\u001b[39m\u001b[34m(self, pointmap, mask)\u001b[39m\n\u001b[32m    547\u001b[39m mask_bool = mask_resized.reshape(-\u001b[32m1\u001b[39m) > \u001b[32m0.5\u001b[39m\n\u001b[32m    548\u001b[39m mask_points = pointmap_flat[:, mask_bool]\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmask_points\u001b[49m\u001b[43m.\u001b[49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m == \u001b[32m0\u001b[39m:\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raise_on_no_valid_points:\n\u001b[32m    552\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo valid points found in mask\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sam3d-objects/lib/python3.11/site-packages/torch/utils/_device.py:106\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    105\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument."
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"../../../../../projects/FRI/jn16867/3d-counting/scenes_part1\")\n",
    "\n",
    "print(f\"Preparing data\")\n",
    "start_time = time.time()\n",
    "\n",
    "prepare_data(data_dir=data_dir, pipeline=pipeline)\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print(f\"Finished preparing data, time taken: {end_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 19:13:30.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mmerge_image_and_mask\u001b[0m:\u001b[36m584\u001b[0m - \u001b[1mReplacing alpha channel with the provided mask\u001b[0m\n",
      "\u001b[32m2026-02-20 19:13:33.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mget_condition_input\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1mRunning condition embedder ...\u001b[0m\n",
      "\u001b[32m2026-02-20 19:13:33.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mget_condition_input\u001b[0m:\u001b[36m637\u001b[0m - \u001b[1mCondition embedder finishes!\u001b[0m\n",
      "\u001b[32m2026-02-20 19:13:47.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mget_condition_input\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1mRunning condition embedder ...\u001b[0m\n",
      "\u001b[32m2026-02-20 19:13:47.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mget_condition_input\u001b[0m:\u001b[36m637\u001b[0m - \u001b[1mCondition embedder finishes!\u001b[0m\n",
      "\u001b[32m2026-02-20 19:14:05.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mdecode_slat\u001b[0m:\u001b[36m606\u001b[0m - \u001b[1mDecoding sparse latent...\u001b[0m\n",
      "\u001b[32m2026-02-20 19:14:14.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mpostprocess_slat_output\u001b[0m:\u001b[36m540\u001b[0m - \u001b[1mPostprocessing mesh with option with_mesh_postprocess False, with_texture_baking False...\u001b[0m\n",
      "\u001b[32m2026-02-20 19:14:14.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mmerge_image_and_mask\u001b[0m:\u001b[36m584\u001b[0m - \u001b[1mReplacing alpha channel with the provided mask\u001b[0m\n",
      "\u001b[32m2026-02-20 19:14:14.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mget_condition_input\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1mRunning condition embedder ...\u001b[0m\n",
      "\u001b[32m2026-02-20 19:14:14.437\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mget_condition_input\u001b[0m:\u001b[36m637\u001b[0m - \u001b[1mCondition embedder finishes!\u001b[0m\n",
      "\u001b[32m2026-02-20 19:14:27.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mget_condition_input\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1mRunning condition embedder ...\u001b[0m\n",
      "\u001b[32m2026-02-20 19:14:27.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mget_condition_input\u001b[0m:\u001b[36m637\u001b[0m - \u001b[1mCondition embedder finishes!\u001b[0m\n",
      "\u001b[32m2026-02-20 19:14:33.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mdecode_slat\u001b[0m:\u001b[36m606\u001b[0m - \u001b[1mDecoding sparse latent...\u001b[0m\n",
      "\u001b[32m2026-02-20 19:14:34.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mpostprocess_slat_output\u001b[0m:\u001b[36m540\u001b[0m - \u001b[1mPostprocessing mesh with option with_mesh_postprocess False, with_texture_baking False...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings for 00000111_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 19:14:35.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mmerge_image_and_mask\u001b[0m:\u001b[36m584\u001b[0m - \u001b[1mReplacing alpha channel with the provided mask\u001b[0m\n",
      "\u001b[32m2026-02-20 19:14:35.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mget_condition_input\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1mRunning condition embedder ...\u001b[0m\n",
      "\u001b[32m2026-02-20 19:14:35.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mget_condition_input\u001b[0m:\u001b[36m637\u001b[0m - \u001b[1mCondition embedder finishes!\u001b[0m\n",
      "\u001b[32m2026-02-20 19:14:48.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mget_condition_input\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1mRunning condition embedder ...\u001b[0m\n",
      "\u001b[32m2026-02-20 19:14:48.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mget_condition_input\u001b[0m:\u001b[36m637\u001b[0m - \u001b[1mCondition embedder finishes!\u001b[0m\n",
      "\u001b[32m2026-02-20 19:14:59.543\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mdecode_slat\u001b[0m:\u001b[36m606\u001b[0m - \u001b[1mDecoding sparse latent...\u001b[0m\n",
      "\u001b[32m2026-02-20 19:15:01.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mpostprocess_slat_output\u001b[0m:\u001b[36m540\u001b[0m - \u001b[1mPostprocessing mesh with option with_mesh_postprocess False, with_texture_baking False...\u001b[0m\n",
      "\u001b[32m2026-02-20 19:15:01.543\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mmerge_image_and_mask\u001b[0m:\u001b[36m584\u001b[0m - \u001b[1mReplacing alpha channel with the provided mask\u001b[0m\n",
      "\u001b[32m2026-02-20 19:15:01.727\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mget_condition_input\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1mRunning condition embedder ...\u001b[0m\n",
      "\u001b[32m2026-02-20 19:15:01.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mget_condition_input\u001b[0m:\u001b[36m637\u001b[0m - \u001b[1mCondition embedder finishes!\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m box_mask = (box_mask > \u001b[32m0\u001b[39m).astype(np.uint8) * \u001b[32m255\u001b[39m\n\u001b[32m     13\u001b[39m object_out = pipeline.run_with_embeddings(image, obj_mask, seed=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m container_out = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_with_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(scene / \u001b[33m\"\u001b[39m\u001b[33mgt_count.json\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     17\u001b[39m     gt_count = json.load(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cso/sam-3d-objects/sam3d_objects/pipeline/inference_with_embeddings.py:154\u001b[39m, in \u001b[36mInferenceWithEmbeddings.run_with_embeddings\u001b[39m\u001b[34m(self, image, mask, seed, stage1_inference_steps, stage2_inference_steps)\u001b[39m\n\u001b[32m    151\u001b[39m slat_input_dict = \u001b[38;5;28mself\u001b[39m._pipeline.preprocess_image(image, \u001b[38;5;28mself\u001b[39m._pipeline.slat_preprocessor, pointmap=pointmap)\n\u001b[32m    152\u001b[39m torch.manual_seed(seed)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m ss_return_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample_sparse_structure_with_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mss_input_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m    \u001b[49m\u001b[43minference_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstage1_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m ss_return_dict.update(\u001b[38;5;28mself\u001b[39m._pipeline.pose_decoder(ss_return_dict))\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mscale\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ss_return_dict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cso/sam-3d-objects/sam3d_objects/pipeline/inference_with_embeddings.py:227\u001b[39m, in \u001b[36mInferenceWithEmbeddings._sample_sparse_structure_with_embeddings\u001b[39m\u001b[34m(self, ss_input_dict, inference_steps, use_distillation)\u001b[39m\n\u001b[32m    220\u001b[39m condition_args, condition_kwargs = \u001b[38;5;28mself\u001b[39m._pipeline.get_condition_input(\n\u001b[32m    221\u001b[39m     \u001b[38;5;28mself\u001b[39m._pipeline.condition_embedders[\u001b[33m\"\u001b[39m\u001b[33mss_condition_embedder\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    222\u001b[39m     ss_input_dict,\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._pipeline.ss_condition_input_mapping,\n\u001b[32m    224\u001b[39m )\n\u001b[32m    225\u001b[39m condition_embedding = condition_args[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m condition_args \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m return_dict = \u001b[43mss_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlatent_shape_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mcondition_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcondition_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pipeline.is_mm_dit():\n\u001b[32m    234\u001b[39m     return_dict = {\u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m: return_dict}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sam3d-objects/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sam3d-objects/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cso/sam-3d-objects/sam3d_objects/model/backbone/generator/base.py:38\u001b[39m, in \u001b[36mBase.forward\u001b[39m\u001b[34m(self, x_shape, x_device, *args_conditionals, **kwargs_conditionals)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_shape, x_device, *args_conditionals, **kwargs_conditionals):\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_conditionals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_conditionals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cso/sam-3d-objects/sam3d_objects/model/backbone/generator/base.py:46\u001b[39m, in \u001b[36mBase.generate\u001b[39m\u001b[34m(self, x_shape, x_device, *args_conditionals, **kwargs_conditionals)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_shape, x_device, *args_conditionals, **kwargs_conditionals):\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_iter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_conditionals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_conditionals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m xt\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cso/sam-3d-objects/sam3d_objects/model/backbone/generator/shortcut/model.py:425\u001b[39m, in \u001b[36mShortCut.generate_iter\u001b[39m\u001b[34m(self, x_shape, x_device, *args_conditionals, **kwargs_conditionals)\u001b[39m\n\u001b[32m    422\u001b[39m x_0 = \u001b[38;5;28mself\u001b[39m._generate_noise(x_shape, x_device)\n\u001b[32m    423\u001b[39m t_seq, d = \u001b[38;5;28mself\u001b[39m._prepare_t_and_d()\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_solver\u001b[49m\u001b[43m.\u001b[49m\u001b[43msolve_iter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_dynamics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_conditionals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_conditionals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cso/sam-3d-objects/sam3d_objects/model/backbone/generator/flow_matching/solver.py:37\u001b[39m, in \u001b[36mODESolver.solve_iter\u001b[39m\u001b[34m(self, dynamics_fn, x_init, times, *args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t0, t1 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(times[:-\u001b[32m1\u001b[39m], times[\u001b[32m1\u001b[39m:]):\n\u001b[32m     36\u001b[39m     dt = t1 - t0\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     x_t = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdynamics_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m x_t, t0\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cso/sam-3d-objects/sam3d_objects/model/backbone/generator/flow_matching/solver.py:49\u001b[39m, in \u001b[36mEuler.step\u001b[39m\u001b[34m(self, dynamics_fn, x_t, t, dt, *args, **kwargs)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, dynamics_fn, x_t, t, dt, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     velocity = \u001b[43mdynamics_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     x_tp1 = linear_approximation_step(x_t, dt, velocity)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x_tp1\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cso/sam-3d-objects/sam3d_objects/model/backbone/generator/shortcut/model.py:444\u001b[39m, in \u001b[36mShortCut._generate_dynamics\u001b[39m\u001b[34m(self, x_t, t, d, *args_conditionals, **kwargs_conditionals)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_dynamics\u001b[39m(\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    437\u001b[39m     x_t,\n\u001b[32m   (...)\u001b[39m\u001b[32m    441\u001b[39m     **kwargs_conditionals,\n\u001b[32m    442\u001b[39m ):\n\u001b[32m    443\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate dynamics for ODE solver\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m     t = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtime_scale\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_get_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m     d = torch.tensor(\n\u001b[32m    448\u001b[39m         [d * \u001b[38;5;28mself\u001b[39m.time_scale], device=_get_device(x_t), dtype=torch.float32\n\u001b[32m    449\u001b[39m     )\n\u001b[32m    450\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reverse_fn(x_t, t, *args_conditionals, d=d, **kwargs_conditionals)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sam3d-objects/lib/python3.11/site-packages/torch/utils/_device.py:106\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    105\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for scene in sorted(data_dir.iterdir()):\n",
    "    if not os.path.exists(scene / \"geco2_data\"):\n",
    "        print(f\"Skipping scene {scene}, no geco2_data found\")\n",
    "        continue\n",
    "    \n",
    "    image = np.array(Image.open(scene / \"geco2_data\" / \"image.png\").convert(\"RGBA\"))\n",
    "    obj_mask = np.array(Image.open(scene / \"geco2_data\" / \"obj_mask.png\").convert(\"L\"))\n",
    "    box_mask = np.array(Image.open(scene / \"geco2_data\" / \"box_mask.png\").convert(\"L\"))\n",
    "    \n",
    "    obj_mask = (obj_mask > 0).astype(np.uint8) * 255\n",
    "    box_mask = (box_mask > 0).astype(np.uint8) * 255\n",
    "    \n",
    "    object_out = pipeline.run_with_embeddings(image, obj_mask, seed=42)\n",
    "    container_out = pipeline.run_with_embeddings(image, box_mask, seed=42)\n",
    "    \n",
    "    with open(scene / \"gt_count.json\") as f:\n",
    "        gt_count = json.load(f)\n",
    "    \n",
    "    save_dict = {\n",
    "        \"container\": container_out,\n",
    "        \"object\": object_out,\n",
    "        \"true_count\": gt_count\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, scene / \"embeddings.pt\")\n",
    "    print(f\"Saved embeddings for {scene.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 1325\n"
     ]
    }
   ],
   "source": [
    "print(obj_mask.min(), obj_mask.max(), obj_mask.sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sam3d-objects)",
   "language": "python",
   "name": "sam3d-objects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f827bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ.pop(\"SSL_CERT_FILE\", None)\n",
    "os.environ.pop(\"SSL_CERT_DIR\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b50ff-9dd6-4689-9669-607ce12bdb2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gradio as gr\n",
    "from gradio_image_prompter import ImagePrompter\n",
    "from torch.nn import DataParallel\n",
    "from models.counter_infer import build_model\n",
    "from utils.arg_parser import get_argparser\n",
    "from utils.data import resize_and_pad\n",
    "import torchvision.ops as ops\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac85be-6f24-409f-be56-c6e23b090d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model (once, to avoid reloading)\n",
    "args = get_argparser().parse_known_args()[0]\n",
    "args.zero_shot = False\n",
    "model = DataParallel(build_model(args).to(device))\n",
    "model.load_state_dict(torch.load('CNTQG_multitrain_ca44.pth', weights_only=True)['model'], strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **Function to Process Image Once**\n",
    "def process_image_once(inputs, enable_mask):\n",
    "    model.module.return_masks = enable_mask\n",
    "\n",
    "    image = inputs['image']\n",
    "    drawn_boxes = inputs['points']\n",
    "    image_tensor = torch.tensor(image).to(device)\n",
    "    image_tensor = image_tensor.permute(2, 0, 1).float() / 255.0\n",
    "    image_tensor = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image_tensor)\n",
    "\n",
    "    bboxes_tensor = torch.tensor([[box[0], box[1], box[2], box[3]] for box in drawn_boxes], dtype=torch.float32).to(\n",
    "        device)\n",
    "\n",
    "    img, bboxes, scale = resize_and_pad(image_tensor, bboxes_tensor, size=1024.0)\n",
    "    img = img.unsqueeze(0).to(device)\n",
    "    bboxes = bboxes.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.cuda()\n",
    "        outputs, _, _, _, masks = model(img.to(device), bboxes.to(device))\n",
    "\n",
    "    # move ALL outputs to CPU, key-by-key (handles lists/dicts safely)\n",
    "    outputs = [\n",
    "        {k: (v.detach().cpu() if torch.is_tensor(v) else v) for k, v in out.items()}\n",
    "        for out in outputs\n",
    "    ]\n",
    "\n",
    "    # make sure masks is on CPU and in a consistent structure\n",
    "    if enable_mask and masks is not None:\n",
    "        if torch.is_tensor(masks):\n",
    "            masks = masks.detach().cpu()\n",
    "        elif isinstance(masks, (list, tuple)):\n",
    "            masks = [m.detach().cpu() for m in masks]\n",
    "    else:\n",
    "        masks = None\n",
    "\n",
    "    return image, outputs, masks, img, scale, drawn_boxes\n",
    "\n",
    "\n",
    "# **Post-process and Update Output**\n",
    "def post_process(image, outputs, masks, img, scale, drawn_boxes, enable_mask, threshold):\n",
    "    idx = 0\n",
    "    thr_inv = 1.0 / threshold  # keep your original intent\n",
    "\n",
    "    # --- pull tensors & drop batch dim if present ---\n",
    "    pred_boxes = outputs[idx]['pred_boxes']          # [1, N, 4] or [N, 4]\n",
    "    box_v      = outputs[idx]['box_v']               # [1, N]    or [N]\n",
    "\n",
    "    if pred_boxes.dim() == 3 and pred_boxes.size(0) == 1:\n",
    "        pred_boxes = pred_boxes[0]                   # -> [N, 4]\n",
    "    if box_v.dim() == 2 and box_v.size(0) == 1:\n",
    "        box_v = box_v[0]                             # -> [N]\n",
    "\n",
    "    # --- selection mask over N ---\n",
    "    sel = box_v > (box_v.max() / thr_inv)            # [N] bool\n",
    "\n",
    "    # handle no survivors cleanly\n",
    "    if sel.sum().item() == 0:\n",
    "        # just draw the user boxes and 0 count\n",
    "        image_pil = Image.fromarray(image.astype(np.uint8))\n",
    "        draw = ImageDraw.Draw(image_pil)\n",
    "        for box in drawn_boxes:\n",
    "            draw.rectangle([box[0], box[1], box[2], box[3]], outline=\"red\", width=3)\n",
    "        # counter badge\n",
    "        w, h = image_pil.size\n",
    "        sq = int(0.05 * w)\n",
    "        x1, y1 = 10, h - sq - 10\n",
    "        draw.rectangle([x1, y1, x1+sq, y1+sq], outline=\"black\", fill=\"black\")\n",
    "        font = ImageFont.load_default()\n",
    "        txt = \"0\"\n",
    "        text_x = x1 + (sq - draw.textlength(txt, font=font)) / 2\n",
    "        text_y = y1 + (sq - 10) / 2\n",
    "        draw.text((text_x, text_y), txt, fill=\"white\", font=font)\n",
    "        return image_pil, 0\n",
    "\n",
    "    # --- NMS expects [N,4] boxes and [N] scores ---\n",
    "    keep = ops.nms(pred_boxes[sel], box_v[sel], 0.5)\n",
    "    pred_boxes = pred_boxes[sel][keep]               # [M,4]\n",
    "    box_v = box_v[sel][keep]                         # [M]\n",
    "\n",
    "    # clamp/scale to original image coords\n",
    "    pred_boxes = torch.clamp(pred_boxes, 0, 1)\n",
    "    pred_boxes = (pred_boxes / scale * img.shape[-1]).tolist()\n",
    "\n",
    "    # to PIL\n",
    "    image_pil = Image.fromarray(image.astype(np.uint8))\n",
    "\n",
    "    # --- masks (optional) ---\n",
    "    if enable_mask and masks is not None:\n",
    "        from matplotlib import pyplot as plt\n",
    "        # get batch slice, drop batch dim if present\n",
    "        base = masks[idx]\n",
    "        # If base is a tensor, ensure it has 3 dims [N,H,W]\n",
    "        if torch.is_tensor(base):\n",
    "            if base.dim() == 4 and base.size(0) == 1:  # e.g., [1,N,H,W]\n",
    "                base = base[0]  # -> [N,H,W]\n",
    "        elif isinstance(base, (list, tuple)):\n",
    "            # convert list of N [H,W] masks into [N,H,W] tensor\n",
    "            base = torch.stack(base, dim=0)  # now base is [N,H,W]\n",
    "        else:\n",
    "            raise TypeError(f\"Unexpected mask type: {type(base)}\")\n",
    "\n",
    "\n",
    "        if masks is not None:\n",
    "            masks_ = base[sel][keep]\n",
    "            N_masks = masks_.shape[0]\n",
    "            indices = torch.randint(1, N_masks + 1, (1, N_masks), device=masks_.device).view(-1, 1, 1)\n",
    "            mask_lbl = (masks_ * indices).sum(dim=0) # [H, W]\n",
    "            mask_display = (\n",
    "                T.Resize(\n",
    "                    (int(img.shape[2] / scale), int(img.shape[3] / scale)),\n",
    "                    interpolation=T.InterpolationMode.NEAREST\n",
    "                )(mask_lbl.unsqueeze(0))[0]\n",
    "            )[:image_pil.size[1], :image_pil.size[0]]\n",
    "            \n",
    "            masks_orig_shape = (\n",
    "                T.Resize(\n",
    "                    (int(img.shape[2] / scale), int(img.shape[3] / scale)),\n",
    "                    interpolation=T.InterpolationMode.NEAREST\n",
    "                )(masks_)\n",
    "            )[:, :image_pil.size[1], :image_pil.size[0]]\n",
    "\n",
    "            \n",
    "            cmap = plt.cm.tab20\n",
    "            norm = plt.Normalize(vmin=0, vmax=N_masks)\n",
    "            rgba = cmap(norm(mask_display))\n",
    "            rgba[mask_display == 0, -1] = 0\n",
    "            rgba[mask_display != 0, -1] = 0.5\n",
    "            overlay = Image.fromarray((rgba * 255).astype(np.uint8), mode=\"RGBA\")\n",
    "            image_pil = image_pil.convert(\"RGBA\")\n",
    "            image_pil = Image.alpha_composite(image_pil, overlay)\n",
    "\n",
    "    # --- draw boxes & user input ---\n",
    "    draw = ImageDraw.Draw(image_pil)\n",
    "    for box in pred_boxes:\n",
    "        draw.rectangle([box[0], box[1], box[2], box[3]], outline=\"orange\", width=2)\n",
    "    # for box in drawn_boxes:\n",
    "    #    draw.rectangle([box[0], box[1], box[3], box[4]], outline=\"red\", width=3)\n",
    "\n",
    "    # counter badge\n",
    "    w, h = image_pil.size\n",
    "    sq = int(0.05 * w)\n",
    "    x1, y1 = 10, h - sq - 10\n",
    "    draw.rectangle([x1, y1, x1+sq, y1+sq], outline=\"black\", fill=\"black\")\n",
    "    font = ImageFont.load_default()\n",
    "    txt = str(len(pred_boxes))\n",
    "    text_x = x1 + (sq - draw.textlength(txt, font=font)) / 2\n",
    "    text_y = y1 + (sq - 10) / 2\n",
    "    draw.text((text_x, text_y), txt, fill=\"white\", font=font)\n",
    "\n",
    "    return image_pil, len(pred_boxes), masks_orig_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45546623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def get_bounding_boxes_from_mask(mask, save_dir, use_watershed=False):\n",
    "    \"\"\"\n",
    "    mask: HxW binary numpy array (1 = object, 0 = background)\n",
    "    use_watershed: apply watershed to split touching objects\n",
    "    Returns: list of (x_min, y_min, x_max, y_max)\n",
    "    \"\"\"\n",
    "    if use_watershed:\n",
    "        # Convert mask to uint8\n",
    "        mask_uint8 = (mask * 255).astype(np.uint8)\n",
    "        \n",
    "        # Distance transform\n",
    "        distance = cv2.distanceTransform(mask_uint8, cv2.DIST_L2, 5)\n",
    "        \n",
    "        # Find peaks\n",
    "        local_max = cv2.dilate(distance, np.ones((3,3)))\n",
    "        local_max = (distance == local_max).astype(np.uint8)\n",
    "        \n",
    "        # Connected components as markers\n",
    "        num_markers, markers = cv2.connectedComponents(local_max)\n",
    "        markers = markers.astype(np.int32)  # important for watershed\n",
    "        \n",
    "        # Need a 3-channel image for watershed\n",
    "        mask_bgr = cv2.cvtColor(mask_uint8, cv2.COLOR_GRAY2BGR)\n",
    "        mask_ws = cv2.watershed(mask_bgr, markers)\n",
    "        \n",
    "        # Watershed assigns -1 to boundaries\n",
    "        labels = mask_ws.copy()\n",
    "        labels[labels == -1] = 0\n",
    "    else:\n",
    "        # Simple connected components\n",
    "        num_labels, labels = cv2.connectedComponents(mask.astype(np.uint8))\n",
    "\n",
    "    bboxes = []\n",
    "    # print(f\"save_dir: {save_dir} -> num_markers: {num_markers}\")\n",
    "    \n",
    "    \"\"\"    \n",
    "    debug_dir = Path(\"../3d-counting/data/Stacks-3D-Real/scenes\")\n",
    "    category = Path(\"chickpeas\")\n",
    "    \n",
    "    if save_dir == debug_dir / category:\n",
    "        print(labels.shape)\n",
    "        plt.imshow(labels)\n",
    "        plt.show()\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Skip label 0 (background)\n",
    "    for label_id in range(1, labels.max()+1):\n",
    "        ys, xs = np.where(labels == label_id)\n",
    "        if len(xs) == 0 or len(ys) == 0:\n",
    "            continue\n",
    "        x_min, x_max = xs.min(), xs.max()\n",
    "        y_min, y_max = ys.min(), ys.max()\n",
    "        \n",
    "        w, h = x_max - x_min, y_max - y_min\n",
    "        area = w * h\n",
    "        if 0 < area < 10000:  # tweak these thresholds for your objects\n",
    "            bboxes.append((x_min, y_min, x_max, y_max))\n",
    "        \n",
    "    \n",
    "    if len(bboxes) == 0:\n",
    "        print(f\"No bboxes generated for dir {save_dir}\")\n",
    "    bbox_output_file = save_dir / \"generated_bboxes.txt\"\n",
    "    str_bboxes = [(str(x_min), str(y_min), str(x_max), str(y_max)) for (x_min, y_min, x_max, y_max) in bboxes]\n",
    "\n",
    "    with open(bbox_output_file, \"w\") as file:\n",
    "        for line in str_bboxes:\n",
    "            file.write(\" \".join(line) + \"\\n\")\n",
    "        \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7474bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../3d-counting/data/Stacks-3D-Real/scenes\")\n",
    "\n",
    "    \n",
    "def prepare_seg_image(data_dir, category, image_type,  frame_file):    \n",
    "    image_path = data_dir / category / image_type / frame_file\n",
    "\n",
    "    img_la = np.array(Image.open(image_path).convert(\"LA\"))\n",
    "\n",
    "    mask = img_la[:,:,1]\n",
    "    obj_seg = img_la.copy()[:,:,0]\n",
    "    obj_seg[mask == 0] = 0\n",
    "    \n",
    "    return obj_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa7392",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in os.listdir(data_dir):\n",
    "    frame = \"frame_00001\" # first frame\n",
    "    \n",
    "    obj_seg = prepare_seg_image(data_dir, category, \"obj_seg\", frame + \".png\")\n",
    "    \n",
    "    if obj_seg.any().item() is False:\n",
    "        obj_seg = prepare_seg_image(data_dir, category, \"images\", frame + \".jpg\")    \n",
    "    \n",
    "    bboxes = get_bounding_boxes_from_mask(obj_seg, data_dir / category, use_watershed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866d1875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw image with bounding boxes\n",
    "image_path = data_dir / category / \"images\" / \"frame_00001.jpg\"\n",
    "img_la = np.array(Image.open(image_path).convert(\"LA\"))\n",
    "img_draw = img_la[:,:,:-1].copy()\n",
    "\n",
    "for (x_min, y_min, x_max, y_max) in bboxes:\n",
    "    cv2.rectangle(img_draw, (x_min, y_min), (x_max, y_max), color=(0, 255, 0), thickness=2)\n",
    "    \n",
    "plt.figure()\n",
    "plt.imshow(img_draw)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432d660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show each image with its bounding boxes\n",
    "for category in os.listdir(data_dir):\n",
    "    image_path = data_dir / category / \"images\" / \"frame_00001.jpg\"\n",
    "    img_la = np.array(Image.open(image_path).convert(\"LA\"))\n",
    "    img_draw = img_la[:,:,:-1].copy()\n",
    "\n",
    "    with open(data_dir / category / \"generated_bboxes.txt\", \"r\") as file:\n",
    "        for line in file:\n",
    "            bbox = [int(n) for n in line.strip().split(' ')]\n",
    "            x_min, y_min, x_max, y_max = bbox\n",
    "            cv2.rectangle(img_draw, (x_min, y_min), (x_max, y_max), color=(0, 255, 0), thickness=2)\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.imshow(img_draw)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98377d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_process(ip_data, enable_mask_val, thr):\n",
    "    # ip_data is a dict from ImagePrompter: {'image': np.ndarray, 'points': [...]}\n",
    "    image, outputs, masks, img, scale, drawn_boxes = process_image_once(ip_data, enable_mask_val)\n",
    "    out_img, n, masks_ori = post_process(image, outputs, masks, img, scale, drawn_boxes, enable_mask_val, thr)\n",
    "    return (\n",
    "        out_img, n, masks_ori,  # visible outputs\n",
    "        image, outputs, masks, img, scale, drawn_boxes  # states\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da109b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mask_and_bbox(outputs, masks_tensor, image):\n",
    "    # Extract output sam2 mask and best predicted bbox\n",
    "\n",
    "    masks = np.array(masks_tensor.detach().cpu().numpy())\n",
    "    scores = np.array(outputs[0]['scores'].detach().cpu().numpy())[0]\n",
    "    \n",
    "    N = min(len(scores), masks.shape[0])\n",
    "    scores = scores[:N]\n",
    "    masks = masks[:N, :, :]\n",
    "    \n",
    "    max_scores_ind = np.argmax(scores, axis=-1).item()\n",
    "\n",
    "    # max_ind = np.argmax(np.array(outputs[0]['scores']), axis=-1).item()\n",
    "    # top5_inds = np.argsort(scores[0])[-5:][::-1]\n",
    "\n",
    "    mask = masks[max_scores_ind, :, :].astype(np.uint8) * 255\n",
    "\n",
    "    bboxes_tensor = outputs[0]['pred_boxes'].squeeze(0).detach().cpu().numpy()\n",
    "    bbox_norm = np.array(bboxes_tensor[max_scores_ind])\n",
    "\n",
    "    H, W = image.shape[:2]\n",
    "\n",
    "    x1 = int(bbox_norm[0] * W)\n",
    "    y1 = int(bbox_norm[1] * H)\n",
    "    x2 = int(bbox_norm[2] * W)\n",
    "    y2 = int(bbox_norm[3] * H)\n",
    "\n",
    "    bbox_px = np.array([x1, y1, x2, y2], dtype=np.int32)\n",
    "\n",
    "    return mask, bbox_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the mask and the predicted bbox\n",
    "def save_mask_and_bbox(save_dir, mask, bbox, image):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    np.save(save_dir / \"image.npy\", image)\n",
    "    image_pil = Image.fromarray(image, mode=\"RGBA\")\n",
    "    image_pil.save(save_dir / \"image.png\")\n",
    "\n",
    "    np.save(save_dir / \"mask.npy\", mask)\n",
    "    mask_pil = Image.fromarray(mask, mode=\"L\")\n",
    "    mask_pil.save(save_dir / \"mask.png\")\n",
    "\n",
    "    np.save(save_dir / \"bbox.npy\", bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427579c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mask and bounding box\n",
    "def plot_mask_and_bbox(image, mask, bbox):\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    axs[0].imshow(mask, cmap=\"gray\")\n",
    "    axs[0].set_title(\"Mask\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    img_vis = image.copy()\n",
    "\n",
    "    cv2.rectangle(\n",
    "        img_vis,\n",
    "        (bbox[0], bbox[1]),\n",
    "        (bbox[2], bbox[3]),\n",
    "        color=(0, 255, 0),\n",
    "        thickness=2\n",
    "    )\n",
    "\n",
    "    axs[1].imshow(img_vis)\n",
    "    axs[1].set_title(\"Image + Bounding Box\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5316190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for single image in category\n",
    "category = \"beads\"\n",
    "print(category)\n",
    "\n",
    "image_path = data_dir / category / \"images\" / \"frame_00001.jpg\"\n",
    "img_rgba = np.array(Image.open(image_path).convert(\"RGBA\"))\n",
    "\n",
    "bboxes_file_path = data_dir / category / \"generated_bboxes.txt\"\n",
    "points = []\n",
    "\n",
    "with open(bboxes_file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        points.append([int(n) for n in line.strip().split(' ')])\n",
    "        \n",
    "\n",
    "input = {\n",
    "    'image': img_rgba[:,:,:-1],\n",
    "    'points': points\n",
    "}\n",
    "enable_mask = True\n",
    "threshold = 0.5\n",
    "\n",
    "out_img, n, masks_ori, image, outputs, masks, img, scale, drawn_boxes = initial_process(input, enable_mask, threshold)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3eec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(inference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029cad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for an image in each category and save data\n",
    "\n",
    "for category in os.listdir(data_dir):\n",
    "    print(f\"Processing category {category}\")\n",
    "    image_path = data_dir / category / \"images\" / \"frame_00001.jpg\"\n",
    "    img_rgba = np.array(Image.open(image_path).convert(\"RGBA\"))\n",
    "\n",
    "    bboxes_file_path = data_dir / category / \"generated_bboxes.txt\"\n",
    "    points = []\n",
    "\n",
    "    with open(bboxes_file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            points.append([int(n) for n in line.strip().split(' ')])\n",
    "    \n",
    "    if len(points) == 0:\n",
    "        print(f\"No points found for category {category}, skipping\")\n",
    "        continue\n",
    "    \n",
    "    input = {\n",
    "        'image': img_rgba[:,:,:-1],\n",
    "        'points': points\n",
    "    }\n",
    "    enable_mask = True\n",
    "    threshold = 0.5\n",
    "    try:\n",
    "        out_img, n, masks_ori_tensor, image, outputs, masks, img, scale, drawn_boxes = initial_process(input, enable_mask, threshold)\n",
    "    except:\n",
    "        print(f\"Error processing category {category}\")\n",
    "    \n",
    "    mask, bbox = extract_mask_and_bbox(outputs, masks_ori_tensor, image)\n",
    "    \n",
    "    save_dir = data_dir / category / \"geco2_mask\"\n",
    "    save_mask_and_bbox(save_dir, mask, bbox, img_rgba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geco2)",
   "language": "python",
   "name": "geco2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
